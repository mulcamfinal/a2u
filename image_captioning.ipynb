{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"image_captioning.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMxCKG32KpSut1cBxanLV5T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGNMVMROLsMT","executionInfo":{"status":"ok","timestamp":1632879984932,"user_tz":-540,"elapsed":32762,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"8281c22c-9737-4a49-e08d-3972c9f6ada0"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxRwKHG3LSf_","executionInfo":{"status":"ok","timestamp":1632879984933,"user_tz":-540,"elapsed":11,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}},"outputId":"d68521a9-d590-4867-88e7-214f33ab548e"},"source":["!nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Sep 29 01:46:24 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"rcZVqSnMLMGx","executionInfo":{"status":"ok","timestamp":1632879986041,"user_tz":-540,"elapsed":1113,"user":{"displayName":"­김동후","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05428640934514761408"}}},"source":["import tensorflow as tf\n","\n","# You'll generate plots of attention in order to see which parts of an image\n","# your model focuses on during captioning\n","import matplotlib.pyplot as plt\n","\n","import collections\n","import random\n","import numpy as np\n","import os\n","import time\n","import json\n","from PIL import Image"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nISuek_ALcFT","outputId":"b08784e0-251a-46d8-cd71-9773a8c22c64"},"source":["# Download caption annotation files\n","# annotation_folder = '/annotations/'\n","# if not os.path.exists(os.path.abspath('.') + annotation_folder):\n","#   annotation_zip = tf.keras.utils.get_file('captions.zip',\n","#                                            cache_subdir=os.path.abspath('.'),\n","#                                            origin='http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n","#                                            extract=True)\n","#   annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n","#   os.remove(annotation_zip)\n","\n","# Download image files\n","image_folder = '/train2014/'\n","if not os.path.exists(os.path.abspath('.') + image_folder):\n","  image_zip = tf.keras.utils.get_file('train2014.zip',\n","                                      cache_subdir=os.path.abspath('.'),\n","                                      origin='http://images.cocodataset.org/zips/train2014.zip',\n","                                      extract=True)\n","  PATH = os.path.dirname(image_zip) + image_folder\n","  os.remove(image_zip)\n","else:\n","  PATH = os.path.abspath('.') + image_folder\n","\n","cocofile = '/content/drive/MyDrive/gh/a2u/DATA/MSCOCO_train_val_Korean.json'\n","with open(cocofile, 'r') as f:\n","    annotations_kr = json.load(f)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from http://images.cocodataset.org/zips/train2014.zip\n"," 1934934016/13510573713 [===>..........................] - ETA: 11:50"]}]},{"cell_type":"code","metadata":{"id":"SKlP9qCfMjSZ"},"source":["# 한국어\n","image_path_to_caption = collections.defaultdict(list)\n","for val in annotations_kr:\n","    if 'COCO_val' in val['file_path']:\n","        continue\n","    tmp = []\n","    for cap in val['caption_ko']:\n","        tmp.append(f\"<start> {cap} <end>\")\n","    image_path = '/content/'+ val['file_path']\n","    image_path_to_caption[image_path] = tmp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CxRK1yaTMk_r"},"source":["image_paths = list(image_path_to_caption.keys())\n","random.shuffle(image_paths)\n","\n","# Select the first 6000 image_paths from the shuffled set.\n","# Approximately each image id has 5 captions associated with it, so that will\n","# lead to 30,000 examples.\n","train_image_paths = image_paths[:6000]\n","print(len(train_image_paths))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iv7qzxUVM_Za"},"source":["train_captions = []\n","img_name_vector = []\n","\n","for image_path in train_image_paths:\n","  caption_list = image_path_to_caption[image_path]\n","  train_captions.extend(caption_list)\n","  img_name_vector.extend([image_path] * len(caption_list))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E86TlHn0NAZY"},"source":["Image.open(img_name_vector[0])\n","print(train_captions[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nbIJL2mENEJQ"},"source":["# InceptionV3 사용하여 전처리\n","# 299px x 299px 로 크기 조정\n","# preprocess_input 메서드로 이미지를 사전 처리 -1~1 범위의 픽셀을 포함하도록 정규화 \n","# -> inceptionv3 를 훈련하는데 사용하는 이미지 형식과 일치하기때문\n","\n","def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.image.decode_jpeg(img, channels=3)\n","    img = tf.image.resize(img, (299, 299))\n","    img = tf.keras.applications.inception_v3.preprocess_input(img)\n","    return img, image_path"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOo0yAgbNbTw"},"source":["# inceptionv3 초기화, 사전훈련된 imagenet 웨이트 로드하기\n","\n","image_model = tf.keras.applications.InceptionV3(include_top=False,\n","                                                weights='imagenet')\n","new_input = image_model.input\n","hidden_layer = image_model.layers[-1].output\n","\n","image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Om_wpKWkNiZg"},"source":["!pip install tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvA6_F8DNjXl"},"source":["from tqdm import tqdm\n","# Get unique images\n","encode_train = sorted(set(img_name_vector))\n","\n","# Feel free to change batch_size according to your system configuration\n","image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n","image_dataset = image_dataset.map(\n","  load_image, num_parallel_calls=tf.data.AUTOTUNE).batch(16)\n","\n","for img, path in tqdm(image_dataset):\n","  batch_features = image_features_extract_model(img)\n","  batch_features = tf.reshape(batch_features,\n","                              (batch_features.shape[0], -1, batch_features.shape[3]))\n","\n","  for bf, p in zip(batch_features, path):\n","    path_of_feature = p.numpy().decode(\"utf-8\")\n","    np.save(path_of_feature, bf.numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"97DSbEInNlgG"},"source":["# 캡션 전처리, 토큰화 부분\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMUyHGPANnwD"},"source":["# Find the maximum length of any caption in the dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LWOamSWxNo4f"},"source":["# Choose the top 5000 words from the vocabulary\n","top_k = 5000\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                  oov_token=\"<unk>\",\n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~')\n","tokenizer.fit_on_texts(train_captions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DHQ6dth1NrQB"},"source":["tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3upvSYJ1NsQg"},"source":["# Create the tokenized vectors\n","train_seqs = tokenizer.texts_to_sequences(train_captions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GYEyodyNtI6"},"source":["# Pad each vector to the max_length of the captions\n","# If you do not provide a max_length value, pad_sequences calculates it automatically\n","cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZRfJ80MNuA6"},"source":["# Calculates the max_length, which is used to store the attention weights\n","max_length = calc_max_length(train_seqs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RlyRruahNu4F"},"source":["# train-val split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2y-TLy-NxuO"},"source":["img_to_cap_vector = collections.defaultdict(list)\n","for img, cap in zip(img_name_vector, cap_vector):\n","  img_to_cap_vector[img].append(cap)\n","\n","# Create training and validation sets using an 80-20 split randomly.\n","img_keys = list(img_to_cap_vector.keys())\n","random.shuffle(img_keys)\n","\n","slice_index = int(len(img_keys)*0.8)\n","img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","\n","img_name_train = []\n","cap_train = []\n","for imgt in img_name_train_keys:\n","  capt_len = len(img_to_cap_vector[imgt])\n","  img_name_train.extend([imgt] * capt_len)\n","  cap_train.extend(img_to_cap_vector[imgt])\n","\n","img_name_val = []\n","cap_val = []\n","for imgv in img_name_val_keys:\n","  capv_len = len(img_to_cap_vector[imgv])\n","  img_name_val.extend([imgv] * capv_len)\n","  cap_val.extend(img_to_cap_vector[imgv])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SV2co5IiN7z0"},"source":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9upLBc6pN-89"},"source":["# 데이터세트 구성"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5jC4ZAXDN9gr"},"source":["# Feel free to change these parameters according to your system's configuration\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","embedding_dim = 256\n","units = 512\n","vocab_size = top_k + 1\n","num_steps = len(img_name_train) // BATCH_SIZE\n","# Shape of the vector extracted from InceptionV3 is (64, 2048)\n","# These two variables represent that vector shape\n","features_shape = 2048\n","attention_features_shape = 64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OHEqcZdDOCt7"},"source":["# Load the numpy files\n","def map_func(img_name, cap):\n","  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n","  return img_tensor, cap"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fyemnjj3ODCJ"},"source":["dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n","\n","# Use map to load the numpy files in parallel\n","dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n","          map_func, [item1, item2], [tf.float32, tf.int32]),\n","          num_parallel_calls=tf.data.AUTOTUNE)\n","\n","# Shuffle and batch\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hybkj0eNOEhx"},"source":["# 모델"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UTFCp3VCOFVR"},"source":["class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, features, hidden):\n","    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","    # hidden shape == (batch_size, hidden_size)\n","    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","\n","    # attention_hidden_layer shape == (batch_size, 64, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                         self.W2(hidden_with_time_axis)))\n","\n","    # score shape == (batch_size, 64, 1)\n","    # This gives you an unnormalized score for each image feature.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights shape == (batch_size, 64, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * features\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJbdqSlnOHHG"},"source":["class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, x):\n","        x = self.fc(x)\n","        x = tf.nn.relu(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0YxtKAwOJHj"},"source":["class RNN_Decoder(tf.keras.Model):\n","  def __init__(self, embedding_dim, units, vocab_size):\n","    super(RNN_Decoder, self).__init__()\n","    self.units = units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc1 = tf.keras.layers.Dense(self.units)\n","    self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","    self.attention = BahdanauAttention(self.units)\n","\n","  def call(self, x, features, hidden):\n","    # defining attention as a separate model\n","    context_vector, attention_weights = self.attention(features, hidden)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # shape == (batch_size, max_length, hidden_size)\n","    x = self.fc1(output)\n","\n","    # x shape == (batch_size * max_length, hidden_size)\n","    x = tf.reshape(x, (-1, x.shape[2]))\n","\n","    # output shape == (batch_size * max_length, vocab)\n","    x = self.fc2(x)\n","\n","    return x, state, attention_weights\n","\n","  def reset_state(self, batch_size):\n","    return tf.zeros((batch_size, self.units))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXM6Op_ZOKQs"},"source":["encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bdZZFnbTOLcd"},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6GNwypPOMKY"},"source":["# cp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jsTPkrdONjN"},"source":["checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ECtEtmeOOYi"},"source":["start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","  # restoring the latest checkpoint in checkpoint_path\n","  ckpt.restore(ckpt_manager.latest_checkpoint)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqxVZ3pOOPXA"},"source":["# train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qdnm3Hj9ORfZ"},"source":["# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","loss_plot = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrDN4vaNOR46"},"source":["@tf.function\n","def train_step(img_tensor, target):\n","  loss = 0\n","\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","\n","  with tf.GradientTape() as tape:\n","      features = encoder(img_tensor)\n","\n","      for i in range(1, target.shape[1]):\n","          # passing the features through the decoder\n","          predictions, hidden, _ = decoder(dec_input, features, hidden)\n","\n","          loss += loss_function(target[:, i], predictions)\n","\n","          # using teacher forcing\n","          dec_input = tf.expand_dims(target[:, i], 1)\n","\n","  total_loss = (loss / int(target.shape[1]))\n","\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","\n","  return loss, total_loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rmxjMcT_OS6x"},"source":["EPOCHS = 20\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    start = time.time()\n","    total_loss = 0\n","\n","    for (batch, (img_tensor, target)) in enumerate(dataset):\n","        batch_loss, t_loss = train_step(img_tensor, target)\n","        total_loss += t_loss\n","\n","        if batch % 100 == 0:\n","            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n","            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n","    # storing the epoch end loss value to plot later\n","    loss_plot.append(total_loss / num_steps)\n","\n","    if epoch % 5 == 0:\n","      ckpt_manager.save()\n","\n","    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n","    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cu7ya2iAOUCK"},"source":["plt.plot(loss_plot)\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss Plot')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yswbiPYVOW4D"},"source":["# caption"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jpK5HTBOYdP"},"source":["def evaluate(image):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","\n","    hidden = decoder.reset_state(batch_size=1)\n","\n","    temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    img_tensor_val = image_features_extract_model(temp_input)\n","    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n","                                                 -1,\n","                                                 img_tensor_val.shape[3]))\n","\n","    features = encoder(img_tensor_val)\n","\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    result = []\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input,\n","                                                         features,\n","                                                         hidden)\n","\n","        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n","\n","        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        result.append(tokenizer.index_word[predicted_id])\n","\n","        if tokenizer.index_word[predicted_id] == '<end>':\n","            return result, attention_plot\n","\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    attention_plot = attention_plot[:len(result), :]\n","    return result, attention_plot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcUh5VkGOZoP"},"source":["def plot_attention(image, result, attention_plot):\n","    temp_image = np.array(Image.open(image))\n","\n","    fig = plt.figure(figsize=(10, 10))\n","\n","    len_result = len(result)\n","    for i in range(len_result):\n","        temp_att = np.resize(attention_plot[i], (8, 8))\n","        grid_size = max(np.ceil(len_result/2), 2)\n","        ax = fig.add_subplot(grid_size, grid_size, i+1)\n","        ax.set_title(result[i])\n","        img = ax.imshow(temp_image)\n","        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n","\n","    plt.tight_layout()\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3BHnXM4DOasw"},"source":["# captions on the validation set\n","rid = np.random.randint(0, len(img_name_val))\n","image = img_name_val[rid]\n","real_caption = ' '.join([tokenizer.index_word[i]\n","                        for i in cap_val[rid] if i not in [0]])\n","result, attention_plot = evaluate(image)\n","\n","print('Real Caption:', real_caption)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image, result, attention_plot)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aO6LQ2KtOcMt"},"source":["image_url = 'https://tensorflow.org/images/surf.jpg'\n","image_extension = image_url[-4:]\n","image_path = tf.keras.utils.get_file('image'+image_extension, origin=image_url)\n","\n","result, attention_plot = evaluate(image_path)\n","print('Prediction Caption:', ' '.join(result))\n","plot_attention(image_path, result, attention_plot)\n","# opening the image\n","Image.open(image_path)"],"execution_count":null,"outputs":[]}]}